#!/usr/bin/env python3
"""
Script principal de anÃ¡lisis de ventas con PySpark
"""

import sys
import os

# Agregar src al path
sys.path.append(os.path.join(os.path.dirname(__file__), './src'))

from config.spark_config import crear_spark_session, detener_spark_session
from etl.transformaciones import TransformacionesVentas

def main():
    """FunciÃ³n principal"""
    
    print("ğŸš€ Iniciando anÃ¡lisis de ventas con PySpark")
    
    # Crear sesiÃ³n de Spark
    spark = crear_spark_session("AnalisisVentas-Produccion")
    
    try:

        # Crear carpeta de resultados
        os.makedirs("resultados", exist_ok=True)

        # Inicializar transformaciones ETL
        transformaciones = TransformacionesVentas(spark)
        
        # Cargar datos
        ventas_df, productos_df = transformaciones.cargar_datos()
        
        print(f"\nğŸ“Š Datos cargados:")
        print(f"   - Ventas: {ventas_df.count()} registros")
        print(f"   - Productos: {productos_df.count()} registros")
        
        # Calcular mÃ©tricas
        ventas_completas_df, metricas_df = transformaciones.calcular_metricas(
            ventas_df, productos_df
        )
        
        # Mostrar resultados
        print("\nğŸ† Top productos por ingresos:")
        metricas_df.show(truncate=False)
        
        # AnÃ¡lisis temporal
        analisis_temporal_df = transformaciones.analisis_temporal(ventas_df)
        
        print("\nğŸ“… Resumen diario:")
        analisis_temporal_df.show()
        
        # Top clientes
        top_clientes_df = transformaciones.top_clientes(ventas_df, top_n=3)
        
        print("\nğŸ‘‘ Top 3 clientes:")
        top_clientes_df.show()
        
        # Guardar resultados
        print("\nğŸ’¾ Guardando resultados...")
        os.makedirs("resultados", exist_ok=True)

        base_path = "file:///home/hadoop/bigdata-spark/resultados"

        metricas_df.write.mode("overwrite").parquet(
            f"{base_path}/metricas_ventas.parquet"
        )

        ventas_completas_df.write.mode("overwrite").parquet(
            f"{base_path}/ventas_completas.parquet"
        )
        
        print("âœ… Resultados guardados en carpeta 'resultados/'")
        
    except Exception as e:
        print(f"âŒ Error durante la ejecucion: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        # Detener sesiÃ³n de Spark
        detener_spark_session(spark)
        print("ğŸ”´ SesiÃ³n de Spark detenida correctamente")

if __name__ == "__main__":
    main()
